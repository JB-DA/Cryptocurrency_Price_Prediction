{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "intimate-stranger",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-24T13:55:48.425586Z",
     "start_time": "2021-02-24T13:55:44.006893Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "### IMPORTS\n",
    "#\n",
    "from flask import Flask, jsonify, render_template, request\n",
    "from pandas.plotting import lag_plot\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sqlalchemy import create_engine\n",
    "from sqlite3 import Error\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.arima_model import ARIMAResults\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import dataframe_image as dfi\n",
    "import datetime\n",
    "import glob\n",
    "import joblib\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlite3\n",
    "import time\n",
    "# END IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "honest-meter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-24T13:56:09.915891Z",
     "start_time": "2021-02-24T13:56:08.872975Z"
    },
    "code_folding": [
     99
    ]
   },
   "outputs": [],
   "source": [
    "### VARIABLES\n",
    "#\n",
    "# General\n",
    "database = r'crypto_db.sqlite'\n",
    "coins = ['ETH']\n",
    "apikeys = 'EFA335E9-DA1A-42CC-A498-DFC46281CE85','1C124E08-4D3D-45BC-9EB5-AB1AC5206B47','B44F0242-E0BA-4C1A-BED2-831A67426480', '1830D89F-A633-4F73-9707-3A7FAFE5C0F0', '200EF4DD-8BF3-4A8A-9FC9-CF9C9D6D1173'\n",
    "\n",
    "# Database Schema\n",
    "sql_create_assets_table = \"\"\" CREATE TABLE IF NOT EXISTS \"assets\" (\n",
    "    \"asset_id\" VARCHAR   NOT NULL,\n",
    "    \"name\" VARCHAR   NOT NULL,\n",
    "    \"type_is_crypto\" INT   NOT NULL,\n",
    "    \"data_quote_start\" VARCHAR   NOT NULL,\n",
    "    \"data_quote_end\" VARCHAR   NOT NULL,\n",
    "    \"data_orderbook_start\" VARCHAR   NOT NULL,\n",
    "    \"data_orderbook_end\" VARCHAR   NOT NULL,\n",
    "    \"data_trade_start\" VARCHAR   NOT NULL,\n",
    "    \"data_trade_end\" VARCHAR   NOT NULL,\n",
    "    \"data_quote_count\" VARCHAR   NOT NULL,\n",
    "    \"data_trade_count\" VARCHAR   NOT NULL,\n",
    "    \"data_symbols_count\" INT   NOT NULL,\n",
    "    \"volume_1hrs_usd\" FLOAT   NOT NULL,\n",
    "    \"volume_1day_usd\" FLOAT   NOT NULL,\n",
    "    \"volume_1mth_usd\" FLOAT   NOT NULL,\n",
    "    \"price_usd\" FLOAT   NOT NULL,\n",
    "    PRIMARY KEY (\"asset_id\"),\n",
    "    FOREIGN KEY (\"asset_id\") REFERENCES \"historic_data\" (\"asset_id\")\n",
    ");\"\"\"\n",
    "\n",
    "sql_create_exchanges_table = \"\"\" CREATE TABLE IF NOT EXISTS \"exchanges\" (\n",
    "    \"exchange_id\" VARCHAR   NOT NULL,\n",
    "    \"website\" VARCHAR   NOT NULL,\n",
    "    \"name\" VARCHAR   NOT NULL,\n",
    "    \"data_start\" VARCHAR   NOT NULL,\n",
    "    \"data_end\" VARCHAR   NOT NULL,\n",
    "    \"data_quote_start\" VARCHAR   NOT NULL,\n",
    "    \"data_quote_end\" VARCHAR   NOT NULL,\n",
    "    \"data_orderbook_start\" VARCHAR   NOT NULL,\n",
    "    \"data_orderbook_end\" VARCHAR   NOT NULL,\n",
    "    \"data_trade_start\" VARCHAR   NOT NULL,\n",
    "    \"data_trade_end\" VARCHAR   NOT NULL,\n",
    "    \"data_symbols_count\" INT   NOT NULL,\n",
    "    \"volume_1hrs_usd\" FLOAT   NOT NULL,\n",
    "    \"volume_1day_usd\" FLOAT   NOT NULL,\n",
    "    \"volume_1mth_usd\" FLOAT   NOT NULL\n",
    ");\"\"\"\n",
    "\n",
    "sql_create_exchange_rates_table = \"\"\" CREATE TABLE IF NOT EXISTS \"exchange_rates\" (\n",
    "    \"time\" VARCHAR   NOT NULL,\n",
    "    \"asset_id_quote\" VARCHAR   NOT NULL,\n",
    "    \"rate\" FLOAT   NOT NULL,\n",
    "    FOREIGN KEY (\"asset_id_quote\") REFERENCES \"assets\" (\"asset_id\")\n",
    ");\"\"\"\n",
    "\n",
    "sql_create_periods_table = \"\"\" CREATE TABLE IF NOT EXISTS \"periods\" (\n",
    "    \"period_id\" VARCHAR   NOT NULL,\n",
    "    \"length_seconds\" INT   NOT NULL,\n",
    "    \"length_months\" INT   NOT NULL,\n",
    "    \"unit_count\" INT   NOT NULL,\n",
    "    \"unit_name\" VARCHAR   NOT NULL,\n",
    "    \"display_name\" VARCHAR   NOT NULL\n",
    ");\"\"\"\n",
    "\n",
    "sql_create_historic_data_table = \"\"\" CREATE TABLE IF NOT EXISTS \"historic_data\" (\n",
    "    \"asset_id\" VARCHAR  NOT NULL,\n",
    "    \"time_period_start\" VARCHAR   NOT NULL,\n",
    "    \"time_period_end\" VARCHAR   NOT NULL,\n",
    "    \"time_open\" VARCHAR   NULL,\n",
    "    \"time_close\" VARCHAR   NULL,\n",
    "    \"price_open\" FLOAT   NOT NULL,\n",
    "    \"price_high\" FLOAT   NOT NULL,\n",
    "    \"price_low\" FLOAT   NOT NULL,\n",
    "    \"price_close\" FLOAT   NOT NULL,\n",
    "    \"volume_traded\" FLOAT   NOT NULL,\n",
    "    \"trades_count\" INT   NULL\n",
    ");\"\"\"\n",
    "time.sleep(1)\n",
    "# END VARIABLES\n",
    "\n",
    "\n",
    "### DATABASE CONNECTION\n",
    "#\n",
    "def create_connection(db_file):\n",
    "    \"\"\" create a database connection to the SQLite database specified by db_file\n",
    "    :param db_file: database file\n",
    "    :return: Connection object or None\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        return conn\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    return conn\n",
    "# END create_connection\n",
    "\n",
    "\n",
    "### RUN SQL COMMAND\n",
    "#\n",
    "def execute_sql_cmd(conn, command):\n",
    "    \"\"\" run a sql command statement\n",
    "    :param conn: Connection object\n",
    "    :param execute_sql_cmd: run sql statement\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        c = conn.cursor()\n",
    "        c.execute(command)\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "# END execute_sql_cmd\n",
    "\n",
    "\n",
    "### LOAD SCHEMA\n",
    "#\n",
    "conn = create_connection(database)\n",
    "\n",
    "if conn is not None:\n",
    "    execute_sql_cmd(conn, sql_create_assets_table)\n",
    "    execute_sql_cmd(conn, sql_create_exchanges_table)\n",
    "    execute_sql_cmd(conn, sql_create_exchange_rates_table)\n",
    "    execute_sql_cmd(conn, sql_create_periods_table)\n",
    "    execute_sql_cmd(conn, sql_create_historic_data_table)\n",
    "else:\n",
    "    print(\"Error! cannot create the database connection.\")\n",
    "conn.close()\n",
    "# END LOAD SCHEMA\n",
    "\n",
    "\n",
    "### ETL ALL CSVS in 'data_raw'\n",
    "#\n",
    "def data_csv_load():\n",
    "    \n",
    "    csvs = glob.glob('data_raw/*.csv')\n",
    "\n",
    "    for csv in csvs:\n",
    "        df_csv = pd.read_csv(csv)\n",
    "        df_csv.dropna(inplace = True)\n",
    "        df_csv = df_csv.rename(columns = {\n",
    "            df_csv.columns[df_csv.columns.str.contains(pat = 'date', case = False)][0]: 'time_period_end',\n",
    "            df_csv.columns[df_csv.columns.str.contains(pat = 'open', case = False)][0]: 'price_open',\n",
    "            df_csv.columns[df_csv.columns.str.contains(pat = 'high', case = False)][0]: 'price_high',\n",
    "            df_csv.columns[df_csv.columns.str.contains(pat = 'low', case = False)][0]: 'price_low',\n",
    "            df_csv.columns[df_csv.columns.str.contains(pat = 'close', case = False)][0]: 'price_close',\n",
    "            df_csv.columns[df_csv.columns.str.contains(pat = 'vol', case = False)][0]: 'volume_traded',\n",
    "        })\n",
    "        df_csv['asset_id'] = csvs[csvs.index(csv)].split('\\\\', 1)[1].split('_', 2)[0]\n",
    "        df_csv['time_period_end'] = pd.to_datetime(df_csv['time_period_end'], format = '%d/%m/%Y')\n",
    "        df_csv['time_period_start'] = df_csv['time_period_end'].copy()\n",
    "\n",
    "        df_csv = df_csv[['asset_id','time_period_start','time_period_end','price_open','price_high','price_low','price_close','volume_traded']].copy()\n",
    "\n",
    "        #with open(f'data_raw/{df_csv.asset_id[0]}.json', 'w') as f:\n",
    "        #    f.write(df_csv.to_json())\n",
    "\n",
    "        conn = create_connection(database)\n",
    "        df_csv.to_sql('historic_data', conn, if_exists = 'append', index = False) ##switch to append once dev finished\n",
    "        conn.close()\n",
    "# END data_csv_load\n",
    "\n",
    "\n",
    "### ETL ALL API CALL DATA\n",
    "#\n",
    "def data_api_load():\n",
    "        \n",
    "    conn = create_connection(database)\n",
    "    \n",
    "    endpoint = 'https://rest.coinapi.io/v1'\n",
    "    asset_id_base = 'USD'\n",
    "    \n",
    "    # ASSETS TABLE\n",
    "    url = f'{endpoint}/assets'\n",
    "    headers = {'X-CoinAPI-Key': apikeys[0]}\n",
    "    response = requests.get(url, headers = headers)    \n",
    "    #with open('data_raw/assets.json', 'w') as ii:\n",
    "    #    json.dump(response.json(), ii)\n",
    "    pd.DataFrame(response.json()).to_sql(\"assets\", conn, if_exists = 'replace', index = False)\n",
    "    \n",
    "    # EXCHANGES TABLE\n",
    "    url = f'{endpoint}/exchanges'\n",
    "    headers = {'X-CoinAPI-Key': apikeys[0]}\n",
    "    response = requests.get(url, headers = headers)\n",
    "    #with open(f'data_raw/exchanges.json', 'w') as ii:\n",
    "    #    json.dump(response.json(), ii)\n",
    "    pd.DataFrame(response.json()).to_sql(\"exchanges\", conn, if_exists = 'replace', index = False)\n",
    "    \n",
    "    # EXCHANGE RATES TABLE\n",
    "    url = f'{endpoint}/exchangerate/{asset_id_base}'\n",
    "    headers = {'X-CoinAPI-Key': apikeys[0]}\n",
    "    response = requests.get(url, headers = headers)\n",
    "    #with open(f'data_raw/exchange_rates.json', 'w') as ii:\n",
    "    #    json.dump(response.json(), ii)\n",
    "    pd.DataFrame(response.json()['rates']).to_sql(\"exchange_rates\", conn, if_exists = 'replace', index = False)\n",
    "\n",
    "    # TIME PERIODS TABLE\n",
    "    url = f'{endpoint}/ohlcv/periods'\n",
    "    headers = {'X-CoinAPI-Key': apikeys[0]}\n",
    "    response = requests.get(url, headers = headers)\n",
    "    #with open(f'data_raw/periods.json', 'w') as ii:\n",
    "    #    json.dump(response.json(), ii)\n",
    "    pd.DataFrame(response.json()).to_sql(\"periods\", conn, if_exists = 'replace', index = False)\n",
    "\n",
    "    # CRYPTOCURRENCY    \n",
    "    for coin in coins:\n",
    "        query = f\"SELECT data_start from assets WHERE asset_id='{coin}'\"\n",
    "        if conn is not None:\n",
    "            result = pd.read_sql_query(query,conn)\n",
    "        else:\n",
    "            print(\"Error! cannot create the database connection.\")\n",
    "            \n",
    "        endpoint = 'https://rest.coinapi.io/v1'\n",
    "        asset_id_base = f'{coin}'\n",
    "        asset_id_quote = 'USD'\n",
    "        limit = 5000\n",
    "        time_start = result['data_start'][0]\n",
    "        period_id = '1DAY'\n",
    "        include_empty_items = False\n",
    "\n",
    "        url = f'{endpoint}/ohlcv/{asset_id_base}/{asset_id_quote}/history?period_id={period_id}&time_start={time_start}&limit={limit}&include_empty_items={include_empty_items}'\n",
    "\n",
    "        headers = {'X-CoinAPI-Key': apikeys[coins.index(coin) + 1]}\n",
    "        response = requests.get(url, headers = headers)\n",
    "\n",
    "        #with open(f'data_raw/{asset_id_base}.json', 'w') as ii:\n",
    "        #    json.dump(response.json(), ii)\n",
    "        \n",
    "        df_coin = pd.DataFrame(response.json())\n",
    "        df_coin['asset_id'] = coin\n",
    "\n",
    "        df_coin.to_sql(\"historic_data\", conn, if_exists = \"append\", index = False)\n",
    "    conn.close()        \n",
    "# END data_api_load\n",
    "\n",
    "\n",
    "def model():\n",
    "    pass\n",
    "# END model\n",
    "\n",
    "\n",
    "#data_csv_load()\n",
    "#data_api_load()\n",
    "model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-majority",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-24T13:40:21.468333Z",
     "start_time": "2021-02-24T13:31:21.528538Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def data_model():\n",
    "    #MODEL FUNCTION\n",
    "    plt.rcParams.update({'figure.figsize': (9, 7), 'figure.dpi': 120})\n",
    "    split = 80\n",
    "    maxlags = 6\n",
    "\n",
    "    conn = create_connection(database)\n",
    "    query = f\"SELECT DISTINCT asset_id FROM historic_data\"\n",
    "    if conn is not None:\n",
    "        result = pd.read_sql_query(query,conn)\n",
    "    else:\n",
    "        print(\"Error! cannot create the database connection.\")\n",
    "    conn.close()\n",
    "\n",
    "    assets = result.copy()\n",
    "\n",
    "    for asset in assets['asset_id']:        \n",
    "        try:\n",
    "            os.makedirs(f'models/{asset}')\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "\n",
    "        path = f'models/{asset}/'\n",
    "\n",
    "        conn = create_connection(database)\n",
    "        query = f\"SELECT time_period_end, price_close from historic_data WHERE asset_id='{asset}'\"\n",
    "\n",
    "        if conn is not None:\n",
    "            result = pd.read_sql_query(query,conn)\n",
    "        else:\n",
    "            print(\"Error! cannot create the database connection.\")\n",
    "        conn.close()\n",
    "\n",
    "        df = result.copy()\n",
    "\n",
    "        # DICKEY-FULLER TEST FOR TIME SERIES STATIONARITY, test if data series changes with time\n",
    "        def test_stationarity(timeseries, window = 12, cutoff = 0.05):\n",
    "            # Calculates rolling mean & standard deviation\n",
    "            rolmean = timeseries.rolling(window).mean()\n",
    "            rolstd = timeseries.rolling(window).std()\n",
    "            # Plot rolling statistics\n",
    "            fig = plt.figure(figsize = (12, 8))\n",
    "            orig = plt.plot(timeseries, color = 'blue', label = 'Original')\n",
    "            mean = plt.plot(rolmean, color = 'red', label = 'Rolling Mean')\n",
    "            std = plt.plot(rolstd, color = 'black', label = 'Rolling Std')\n",
    "            plt.legend(loc = 'best')\n",
    "            plt.title(f'Rolling Mean & Standard Deviation of {asset}')\n",
    "            plt.savefig(f'{path}{asset}_mean_std.png')\n",
    "            plt.close()    \n",
    "            # Performs dickey-fuller test for time-series statistics\n",
    "            dftest = adfuller(timeseries, autolag = 'AIC', maxlag = 100) #AIC estimator of prediction error\n",
    "            dfoutput = pd.Series(dftest[0:4], index = [ #Display results\n",
    "                                 'Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
    "            for key, value in dftest[4].items():\n",
    "                dfoutput['Critical Value (%s)' % key] = value #If critical value < test statistics, dataset is stationary\n",
    "                pvalue = dftest[1]\n",
    "            #if pvalue < cutoff: #If pvalue < cutoff (0.05), dataset is stationary\n",
    "            #    print('p-value = %.4f. The series is likely stationary.' % pvalue)\n",
    "            #else:\n",
    "            #    print('p-value = %.4f. The series is likely non-stationary.' % pvalue)\n",
    "            return(dfoutput, cutoff)\n",
    "\n",
    "        fuller_out, cutoff = test_stationarity(df['price_close'])\n",
    "\n",
    "        df_fuller = pd.DataFrame(fuller_out)\n",
    "        df_fuller = df_fuller.rename(columns = {\n",
    "            0 : f'Dickey-Fuller Test Scores: {asset}'})\n",
    "\n",
    "        if df_fuller.iloc[1][0] < cutoff: #If pvalue < 0.05, dataset is stationary\n",
    "            df_fuller.loc['Data Stationary?'] = 'Yes'\n",
    "        else:\n",
    "            df_fuller.loc['Data Stationary?'] = 'No'\n",
    "\n",
    "        df_fuller.dfi.export(f'{path}{asset}_dickeyfuller.png')\n",
    "        \n",
    "        #     Lag plots allow you to check for:\n",
    "        #     Model suitability\n",
    "        #     Outliers (data points with extremely high or low values)\n",
    "        #     Randomness (data without a pattern)\n",
    "        #     Serial correlation (where error terms in a time series transfer from one period to another)\n",
    "        #     Seasonality (periodic fluctuations in time series data that happens at regular periods)\n",
    "\n",
    "        lags = int(df_fuller.loc['#Lags Used'][0])\n",
    "        \n",
    "        # plt.figure()\n",
    "        lag_plot(df['price_close'], lag = lags)\n",
    "        plt.title(f'Lag of timeseries with lag = {lags}: {asset}')\n",
    "        plt.savefig(f'{path}{asset}_real_series_lag.png')\n",
    "        plt.close()\n",
    "\n",
    "        if lags > maxlags: #Sets maximum lag value\n",
    "            lags = maxlags\n",
    "        else:\n",
    "            lags = int(df_fuller.loc['#Lags Used'][0])\n",
    "\n",
    "        # plt.figure()\n",
    "        lag_plot(df['price_close'], lag = lags)\n",
    "        plt.title(f'Lag of timeseries with lag = {lags}: {asset}')\n",
    "        plt.savefig(f'{path}{asset}_capped_series_lag.png')\n",
    "        plt.close()\n",
    "\n",
    "        # PLOTS d value first & second differentiation\n",
    "        # Original Series\n",
    "        fig, axes = plt.subplots(3, 2, sharex = True)\n",
    "        axes[0, 0].plot(df[\"price_close\"].values); axes[0, 0].set_title('Original Series')\n",
    "        plot_acf(df[\"price_close\"].values, ax = axes[0, 1])\n",
    "\n",
    "        # 1st Differencing\n",
    "        axes[1, 0].plot(np.diff(df[\"price_close\"].values)); axes[1, 0].set_title('1st Order Differencing')\n",
    "        df = df.dropna()\n",
    "        plot_acf(np.diff(df[\"price_close\"].values), ax = axes[1, 1])\n",
    "\n",
    "        # 2nd Differencing\n",
    "        axes[2, 0].plot(np.diff(df[\"price_close\"].values)); axes[2, 0].set_title('2nd Order Differencing')\n",
    "        plot_acf(np.diff(df[\"price_close\"].values), ax = axes[2, 1])\n",
    "\n",
    "        plt.suptitle(f'Differencing & Correlation: {asset}')\n",
    "        plt.savefig(f'{path}{asset}_1order_2order.png')\n",
    "        plt.close()    \n",
    "\n",
    "        # CREATE TRAIN & TEST DATA FOR MODELLING\n",
    "        train_data, test_data = df[0:int(len(df) * split / 100)], df[int(len(df) * split / 100):]\n",
    "\n",
    "        training_data = train_data['price_close'].values\n",
    "        test_data = test_data['price_close'].values\n",
    "\n",
    "        history = [x for x in training_data]\n",
    "        model_predictions = []\n",
    "        N_test_observations = len(test_data)\n",
    "\n",
    "        # CREATE MODEL\n",
    "        for time_point in range(N_test_observations):\n",
    "            model = ARIMA(history, order = (lags, 1, 0))\n",
    "            model_fit = model.fit(disp = 0)\n",
    "            output = model_fit.forecast()\n",
    "            yhat = output[0]\n",
    "            model_predictions.append(yhat)\n",
    "            true_test_value = test_data[time_point]\n",
    "            history.append(true_test_value)\n",
    "\n",
    "        model_fit.save(f'{path}{asset}_model.smd')\n",
    "\n",
    "        MSE_error = mean_squared_error(test_data, model_predictions)\n",
    "\n",
    "        # CREATE DATAFRAME WITH DATES, ACTUAL & PREDICTED VALUES\n",
    "        date_df = pd.DataFrame(df[int(len(df) * split / 100):].time_period_end)\n",
    "        df_test = pd.DataFrame(data = test_data)\n",
    "        df_preds = pd.DataFrame(data = model_predictions)\n",
    "\n",
    "        date_df.reset_index(drop = True, inplace = True)\n",
    "        df_test.reset_index(drop = True, inplace = True)\n",
    "        df_preds.reset_index(drop = True, inplace = True)\n",
    "\n",
    "        df_preds = df_preds.shift(periods = -3)\n",
    "\n",
    "        frames = [date_df['time_period_end'], df_test[0], df_preds[0]]\n",
    "        headers = ['Date', 'test', 'preds']\n",
    "        df_graphdata = pd.concat(frames, axis = 1, keys = headers)\n",
    "\n",
    "        # Create Training and Test\n",
    "        split = 85\n",
    "        train = df[0:int(len(df['price_close'])*split/100)].price_close\n",
    "        test = df[int(len(df['price_close'])*split/100):].price_close\n",
    "\n",
    "        # Build Model\n",
    "        # model = ARIMA(train, order=(2,1,0))  \n",
    "        model = ARIMA(np.asarray(train), order = (2, 1, 0))  \n",
    "        fitted = model.fit(disp=0)  \n",
    "\n",
    "        # Forecast\n",
    "        fc, se, conf = fitted.forecast(len(test), alpha = 0.05)  # 95% conf\n",
    "\n",
    "        # Make as pandas series\n",
    "        fc_series = pd.Series(fc, index=test.index)\n",
    "\n",
    "        lower_series = pd.Series(conf[:, 0], index=test.index)\n",
    "        upper_series = pd.Series(conf[:, 1], index=test.index)\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize = (12,5), dpi = 100)\n",
    "        plt.plot(train, label = 'training')\n",
    "        plt.plot(test, label = 'actual')\n",
    "        plt.plot(fc_series, label = 'forecast')\n",
    "        plt.fill_between(lower_series.index, lower_series, upper_series, color = 'k', alpha = 0.15)\n",
    "        plt.title(f'Forecast vs Actuals 1st Order: {asset}')\n",
    "        plt.legend(loc='upper left', fontsize = 8)\n",
    "        plt.savefig(f'{path}{asset}_oot_forecast_order1.png')\n",
    "        plt.close()    \n",
    "\n",
    "        #Build Model with Forecast\n",
    "        # model = ARIMA(train, order=(2,1,0))  \n",
    "        model = ARIMA(np.asarray(train), order = (3, 2, 0))  \n",
    "        fitted = model.fit(disp=0)  \n",
    "\n",
    "        # Forecast\n",
    "        fc, se, conf = fitted.forecast(len(test), alpha = 0.05)  # 95% conf\n",
    "\n",
    "        # Make as pandas series\n",
    "        fc_series = pd.Series(fc, index=test.index)\n",
    "\n",
    "        lower_series = pd.Series(conf[:, 0], index=test.index)\n",
    "        upper_series = pd.Series(conf[:, 1], index=test.index)\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize = (12,5), dpi = 100)\n",
    "        plt.plot(train, label = 'training')\n",
    "        plt.plot(test, label = 'actual')\n",
    "        plt.plot(fc_series, label = 'forecast')\n",
    "        plt.fill_between(lower_series.index, lower_series, upper_series, \n",
    "                         color = 'k', alpha = 0.15)\n",
    "        plt.title(f'Forecast vs Actuals 2nd Order: {asset}')\n",
    "        plt.legend(loc = 'upper left', fontsize = 8)\n",
    "        plt.savefig(f'{path}{asset}_oot_forecast_order2.png')\n",
    "        plt.close()\n",
    "\n",
    "# END data_model\n",
    "data_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-exploration",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-24T10:53:53.732841Z",
     "start_time": "2021-02-24T10:53:53.422Z"
    }
   },
   "outputs": [],
   "source": [
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "\n",
    "# fig = go.Figure([\n",
    "\n",
    "#     go.Scatter(\n",
    "#         name='Actual',\n",
    "#         x=graphdata.Date,\n",
    "#         y=graphdata['test'],\n",
    "#         mode='lines',\n",
    "#         marker=dict(color=\"#008080\"),\n",
    "#         line=dict(width=1),\n",
    "#         showlegend=True\n",
    "#     ),\n",
    "#     go.Scatter(\n",
    "#         name='Prediction',\n",
    "#         x=graphdata.Date,\n",
    "#         y=graphdata['preds'],\n",
    "#         marker=dict(color=\"#FF8C00\"),\n",
    "#         line=dict(width=1),\n",
    "#         mode='lines',\n",
    "#         fillcolor='rgba(68, 68, 68, 0.3)',\n",
    "#         showlegend=True\n",
    "#     )\n",
    "# ])\n",
    "\n",
    "# fig.update_layout(\n",
    "#     yaxis_title='',\n",
    "#     title='',\n",
    "#     hovermode=\"x\"\n",
    "# )\n",
    "\n",
    "# fig.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
